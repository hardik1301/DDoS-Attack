{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Users\\This PC\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-ebc1cd8f9357>:102: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "Training Batch: of Epoch: 1\tLoss: 54142318.385999404\n",
      "Training Batch: of Epoch: 2\tLoss: 8500414.141633652\n",
      "Training Batch: of Epoch: 3\tLoss: 5337426.830470858\n",
      "Training Batch: of Epoch: 4\tLoss: 3485525.8703170526\n",
      "Training Batch: of Epoch: 5\tLoss: 1259578.4967449435\n",
      "Training Batch: of Epoch: 6\tLoss: 929942.8177718214\n",
      "Training Batch: of Epoch: 7\tLoss: 603595.9637606475\n",
      "Training Batch: of Epoch: 8\tLoss: 387163.62682978646\n",
      "Training Batch: of Epoch: 9\tLoss: 144430.18464002883\n",
      "Training Batch: of Epoch: 10\tLoss: 26759.126209442955\n",
      "Training Batch: of Epoch: 11\tLoss: 7501.454382282944\n",
      "Training Batch: of Epoch: 12\tLoss: 15004.967482620239\n",
      "Training Batch: of Epoch: 13\tLoss: 509.7886723736883\n",
      "Training Batch: of Epoch: 14\tLoss: 4865.393760603969\n",
      "Training Batch: of Epoch: 15\tLoss: 6003.636948287487\n",
      "Training Batch: of Epoch: 16\tLoss: 5651.023095935583\n",
      "Training Batch: of Epoch: 17\tLoss: 5820.478186301887\n",
      "Training Batch: of Epoch: 18\tLoss: 5828.585945859551\n",
      "Training Batch: of Epoch: 19\tLoss: 5962.7973921541125\n",
      "Training Batch: of Epoch: 20\tLoss: 6078.199714047834\n",
      "Saved to:  C:\\Users\\This PC\\Documents\\minor sem6\\model_train.ckpt\n",
      "Accuracy  0.888948\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'file://C:\\\\Users\\\\This PC\\\\Documents\\\\minor sem6\\\\graph.html'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from plotly.offline import plot\n",
    "import plotly.graph_objs as ob\n",
    "raw_data = open (r'''IPtrain_data_udp.txt'''). readlines ()\n",
    "raw_labels = open (r'''IPtrain_label_udp.txt'''). readlines ()\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "gra=[]\n",
    "gro=[]\n",
    "\n",
    "for _ in raw_data:\n",
    "\n",
    "\t_ = _.split ()\n",
    "\tx.append ([int (_[0]), int (_[1]), int (_[2]), int (_[3])])\n",
    "\n",
    "for _ in raw_labels:\n",
    "\n",
    "\ty.append ([0, 1])\n",
    "\n",
    "\n",
    "def get_next_batch (batchSize):\n",
    "\n",
    "\t_data = raw_data[get_next_batch.counter : get_next_batch.counter + batchSize]\n",
    "\t_label = raw_labels[get_next_batch.counter : get_next_batch.counter + batchSize]\n",
    "\n",
    "\tget_next_batch.counter += batchSize\n",
    "\n",
    "\tbatch_data = []\n",
    "\tbatch_label = []\n",
    "\n",
    "\tfor _ in _data:\n",
    "\n",
    "\t\t_ = _.split ()\n",
    "\t\tbatch_data.append ([_[0] ,_[1], _[2], _[3]])\n",
    "\t\n",
    "\tfor _ in _label:\n",
    "\n",
    "\t\t_ = _.split ()\n",
    "\t\tbatch_label.append ([_[0], _[1]])\n",
    "\n",
    "\treturn np.array (batch_data), np.array(batch_label)\n",
    "\n",
    "nodesForLayerInput = 4\n",
    "nodesForLayer1 = 50\n",
    "nodesForLayer2 = 50\n",
    "nodesForLayer3 = 50\n",
    "nodesForLayerOut = 1\n",
    "\n",
    "numberOfClassesOut = 2\n",
    "\n",
    "data = tf.placeholder ('float', shape = [None, 4])\n",
    "label = tf.placeholder ('float')\n",
    "\n",
    "layer1 = {\n",
    "\t\t\n",
    "\t\t'w' : tf.Variable (tf.random_normal ([4, nodesForLayer1])),\n",
    "\t\t'b' : tf.Variable (tf.random_normal ([nodesForLayer1]))\n",
    "\t}\n",
    "\n",
    "layer2 = {\n",
    "\t\n",
    "\t'w' : tf.Variable (tf.random_normal ([nodesForLayer1, nodesForLayer2])),\n",
    "\t'b' : tf.Variable (tf.random_normal ([nodesForLayer2]))\n",
    "}\n",
    "\n",
    "layer3 = {\n",
    "\t\n",
    "\t'w' : tf.Variable (tf.random_normal ([nodesForLayer2, nodesForLayer3])),\n",
    "\t'b' : tf.Variable (tf.random_normal ([nodesForLayer3]))\n",
    "}\n",
    "\n",
    "layerOut = {\n",
    "\t\n",
    "\t'w' : tf.Variable (tf.random_normal ([nodesForLayer3, numberOfClassesOut])),\n",
    "\t'b' : tf.Variable (tf.random_normal ([numberOfClassesOut]))\n",
    "}\n",
    "\n",
    "saver = tf.train.Saver ()\n",
    "\n",
    "def graph (_data):\n",
    "\n",
    "\tansLayer1 = tf.nn.relu (tf.add(tf.matmul(_data, layer1['w']), layer1['b']))\n",
    "\tansLayer2 = tf.nn.relu (tf.add(tf.matmul(ansLayer1, layer2['w']), layer2['b']))\n",
    "\tansLayer3 = tf.nn.relu (tf.add(tf.matmul(ansLayer2, layer3['w']), layer3['b']))\n",
    "\n",
    "\tansLayerOut = tf.add(tf.matmul(ansLayer3, layerOut['w']), layerOut['b'])\n",
    "\n",
    "\treturn ansLayerOut\n",
    "\n",
    "def train (_x):\n",
    "\n",
    "\tprediction = graph (_x)\n",
    "\n",
    "\tcost = tf.reduce_mean (tf.nn.softmax_cross_entropy_with_logits (\n",
    "\n",
    "\t\t_sentinel = None,\n",
    "\t\tlogits = prediction,\n",
    "\t\tlabels = label,\n",
    "\t\tdim = -1,\n",
    "\t\tname = None)\n",
    "\t)\n",
    "\n",
    "\toptimiser = tf.train.AdamOptimizer ().minimize (cost)\n",
    "\n",
    "\tnEpochs = 20\n",
    "\n",
    "\twith tf.Session () as sess:\n",
    "\n",
    "\t\tsess.run (tf.global_variables_initializer ())\n",
    "\n",
    "\t\tfor epoch in range (nEpochs):\n",
    "\n",
    "\t\t\tepoch_loss = 0\n",
    "\t\t\ttest=0\n",
    "\t\t\tget_next_batch.counter = 0\n",
    "\n",
    "\t\t\tfor i in range (10000):\n",
    "\n",
    "\t\t\t\tepoch_data, epoch_label = get_next_batch (100)\n",
    "\n",
    "\t\t\t\ti, c = sess.run ([optimiser, cost], feed_dict = {data : epoch_data, label : epoch_label})\n",
    "\n",
    "\t\t\t\tepoch_loss += c\n",
    "\t\t\t\n",
    "\t\t\tprint (\"Training Batch: of Epoch: \" + str (epoch + 1) + \"\\tLoss: \" + str (epoch_loss))\n",
    "\t\t\t\n",
    "\t\t\tgra.append(epoch_loss)\n",
    "\t\t\t\n",
    "\t\tsave_path = saver.save (sess,r'''C:\\Users\\This PC\\Documents\\minor sem6\\model_train.ckpt''')\n",
    "\n",
    "\t\tprint (\"Saved to: \", save_path)\n",
    "\n",
    "\t\tcorrect = tf.equal (tf.argmax (prediction, 1), tf.argmax (label, 1))\n",
    "\n",
    "\t\taccuracy = tf.reduce_mean (tf.cast (correct, 'float'))\n",
    "\n",
    "\t\tprint (\"Accuracy \", accuracy.eval ({data : x, label : y}))\n",
    "\n",
    "train (data)\n",
    "\n",
    "gra1 = list (range (len (gra)))\n",
    "\n",
    "trace0 = ob.Scatter (\n",
    "\t\n",
    "\tx = gra1,\n",
    "\ty = gra,\n",
    "\tmode = 'lines+markers'\n",
    "\t)\n",
    "\n",
    "data = [trace0]\n",
    "\n",
    "plot (data,filename = \"graph.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
